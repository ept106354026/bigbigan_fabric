{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Image-Preprocess-and-Model-Block\" data-toc-modified-id=\"Image-Preprocess-and-Model-Block-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Image Preprocess and Model Block</a></span></li><li><span><a href=\"#Res-Block(option)\" data-toc-modified-id=\"Res-Block(option)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Res Block(option)</a></span></li><li><span><a href=\"#Definite-GAN\" data-toc-modified-id=\"Definite-GAN-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Definite GAN</a></span></li><li><span><a href=\"#Read-and-Generate-Data\" data-toc-modified-id=\"Read-and-Generate-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Read and Generate Data</a></span></li><li><span><a href=\"#Model-Build-and-Training\" data-toc-modified-id=\"Model-Build-and-Training-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Build and Training</a></span></li><li><span><a href=\"#Spectral-Normalization\" data-toc-modified-id=\"Spectral-Normalization-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Spectral Normalization</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocess and Model Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import floor\n",
    "import time\n",
    "\n",
    "from functools import partial,update_wrapper\n",
    "from functools import wraps\n",
    "\n",
    "from random import random\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation,Lambda,GlobalMaxPooling2D,Concatenate,GlobalAveragePooling2D\n",
    "from keras.layers import LeakyReLU,Dot,Add,Layer\n",
    "from keras.layers import Reshape, UpSampling2D, Flatten, Input, add, Lambda, concatenate, LeakyReLU, multiply\n",
    "from keras.layers import Conv2D, Dense, AveragePooling2D, Activation, Cropping2D, Dropout, average\n",
    "from keras.layers import UpSampling2D, Conv2D,Conv2DTranspose,Multiply\n",
    "from keras.layers.merge import _Merge\n",
    "\n",
    "from keras.models import Sequential, Model, model_from_json, Model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras import metrics\n",
    "import keras.backend as K\n",
    "from keras.initializers import VarianceScaling\n",
    "\n",
    "im_size = 128 # image size\n",
    "resize=200 # image resize before cut\n",
    "latent_size = 128 # laten dim\n",
    "BATCH_SIZE = 45 # batch size\n",
    "channels = 3 # image channel\n",
    "d = 32 # attention image size\n",
    "att_fil_div = 8 # attention filter division\n",
    "adam_eps=1e-8 # optimizer adam parameter\n",
    "cha = 16 # base convolution layer filter number\n",
    "\n",
    "# lr = 0.0001\n",
    "# enc_lr = lr*10\n",
    "# gen_lr = lr*1\n",
    "# dis_lr = lr*4\n",
    "\n",
    "def noise(n):\n",
    "    return np.random.normal(0.0, 1.0, size = [n, latent_size])\n",
    "\n",
    "def hinge_d(y_true, y_pred):\n",
    "    return K.mean(K.relu(1.0 - (y_true * y_pred)))\n",
    "\n",
    "def w_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "class Gamma(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Gamma, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(1,),\n",
    "                                      initializer='zeros',\n",
    "                                      trainable=True)\n",
    "        super(Gamma, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return x*self.kernel\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape)\n",
    "    \n",
    "def self_att_block(inp,fil):\n",
    "    n=d**2\n",
    "    query_conv = Conv2D(fil//att_fil_div, 1)(inp)\n",
    "    key_conv = Conv2D(fil//att_fil_div, 1)(inp)\n",
    "    value_conv = Conv2D(fil, 1)(inp)\n",
    "    \n",
    "    query_conv = Reshape((n,-1))(query_conv)\n",
    "    key_conv = Reshape((n,-1))(key_conv)\n",
    "    attention = Dot(-1)([query_conv,key_conv])\n",
    "    attention = Activation('softmax')(attention)\n",
    "    \n",
    "    value_conv = Reshape((n,-1))(value_conv)\n",
    "    value_conv = Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(value_conv)\n",
    "    out = Dot(-1)([attention,value_conv])\n",
    "    out = Reshape((d,d,-1))(out)\n",
    "    \n",
    "    out = Gamma((1,))(out)\n",
    "    out = Add()([inp,out])\n",
    "    \n",
    "    return out\n",
    "\n",
    "def self_att_block_sn(inp,fil):\n",
    "    n=d**2\n",
    "    query_conv = ConvSN2D(fil//att_fil_div, 1)(inp)\n",
    "    key_conv = ConvSN2D(fil//att_fil_div, 1)(inp)\n",
    "    value_conv = ConvSN2D(fil, 1)(inp)\n",
    "    \n",
    "    query_conv = Reshape((n,-1))(query_conv)\n",
    "    key_conv = Reshape((n,-1))(key_conv)\n",
    "    attention = Dot(-1)([query_conv,key_conv])\n",
    "    attention = Activation('softmax')(attention)\n",
    "    \n",
    "    value_conv = Reshape((n,-1))(value_conv)\n",
    "    value_conv = Lambda(lambda x:K.permute_dimensions(x,(0,2,1)))(value_conv)\n",
    "    out = Dot(-1)([attention,value_conv])\n",
    "    out = Reshape((d,d,-1))(out)\n",
    "    out = Gamma((1,))(out)\n",
    "    out = Add()([inp,out])\n",
    "    \n",
    "    return out\n",
    "\n",
    "def g_block(inp, fil):\n",
    "    out = ConvSN2DTranspose(fil, kernel_size=4, strides=2, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    return out\n",
    "\n",
    "def d_block(inp, fil):\n",
    "    out = Conv2D(filters = fil, kernel_size=4, strides=2, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = LeakyReLU(0.1)(out)\n",
    "    return out\n",
    "\n",
    "def dis_block(inp, fil):\n",
    "    out = ConvSN2D(fil, kernel_size=4, strides=2, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "    out = LeakyReLU(0.1)(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Res Block(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_block(inp, fil, u = True):\n",
    "\n",
    "    if u:\n",
    "        out = UpSampling2D(interpolation = 'bilinear')(inp)\n",
    "    else:\n",
    "        out = Activation('linear')(inp)\n",
    "\n",
    "    skip = ConvSN2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "\n",
    "    out = ConvSN2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "#     out = BatchNormalization(trainable=False)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "\n",
    "    out = ConvSN2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "#     out = BatchNormalization(trainable=False)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "\n",
    "    out = ConvSN2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = add([out, skip])\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "#     out = BatchNormalization(trainable=False)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def d_block(inp, fil, p = True):\n",
    "\n",
    "    skip = Conv2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "\n",
    "    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "#     out = BatchNormalization(trainable=False)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "\n",
    "    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "#     out = BatchNormalization(trainable=False)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "\n",
    "\n",
    "    out = Conv2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "\n",
    "    out = add([out, skip])\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "#     out = BatchNormalization(trainable=False)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "\n",
    "    if p:\n",
    "        out = AveragePooling2D()(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def dis_block(inp, fil, p = True):\n",
    "\n",
    "    skip = ConvSN2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "\n",
    "    out = ConvSN2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = ConvSN2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = ConvSN2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "\n",
    "    out = add([out, skip])\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    if p:\n",
    "        out = AveragePooling2D()(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definite GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "\n",
    "    def __init__(self, steps = 1, lr = 0.0001, decay = 0.00001):\n",
    "\n",
    "        #Models\n",
    "        self.D = None\n",
    "        self.E = self.encoder()\n",
    "        self.G = self.generator()\n",
    "\n",
    "        self.GE = self.generator()\n",
    "        self.EE = self.encoder()\n",
    "\n",
    "        self.DM = None\n",
    "        self.AM = None\n",
    "\n",
    "        #Config\n",
    "        self.LR = lr\n",
    "        self.steps = steps\n",
    "        self.beta = 0.999\n",
    "\n",
    "        #Init Models\n",
    "        self.discriminator()\n",
    "        \n",
    "#         self.E.load_weights('sn-sa-bigan-new-128-sfat32-no-res/36000epoch_encoder.h5')\n",
    "#         self.G.load_weights('sn-sa-bigan-new-128-sfat32-no-res/36000epoch_generator.h5')\n",
    "#         self.D.load_weights('sn-sa-bigan-new-128-sfat32-no-res/36000epoch_critic_o.h5')\n",
    "#         self.generator()\n",
    "#         self.encoder()\n",
    "#         self.EE = model_from_json(self.E.to_json())\n",
    "#         self.EE.set_weights(self.E.get_weights())\n",
    "#         self.GE = model_from_json(self.G.to_json())\n",
    "#         self.GE.set_weights(self.G.get_weights())\n",
    "\n",
    "    def discriminator(self):\n",
    "\n",
    "        if self.D:\n",
    "            return self.D\n",
    "\n",
    "        inp = Input(shape = [im_size, im_size, channels])\n",
    "        inpl = Input(shape = [latent_size])\n",
    "\n",
    "        #Latent input\n",
    "        l = DenseSN(512, kernel_initializer = 'he_normal')(inpl)\n",
    "        l = LeakyReLU(0.2)(l)\n",
    "        l = DenseSN(512, kernel_initializer = 'he_normal')(l)\n",
    "        l = LeakyReLU(0.2)(l)\n",
    "        l = DenseSN(512, kernel_initializer = 'he_normal')(l)\n",
    "        l = LeakyReLU(0.2)(l)\n",
    "        sz = Dense(1, kernel_initializer = 'he_normal')(l)\n",
    "\n",
    "        \n",
    "        x = dis_block(inp, 4 * cha)   #64\n",
    "        x = dis_block(x, 8 * cha)  #32\n",
    "        x = self_att_block_sn(x, 8 * cha)\n",
    "        x = dis_block(x, 8 * cha)  #16\n",
    "        x = dis_block(x, 16 * cha)  #8\n",
    "        x = dis_block(x, 16* cha)  #4\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        sx = Dense(1, kernel_initializer = 'he_normal')(x)\n",
    "        xz = concatenate([x, l])\n",
    "        xz = DenseSN(16 * cha, kernel_initializer = 'he_normal')(xz)\n",
    "        xz = LeakyReLU(0.2)(xz)\n",
    "\n",
    "        sxz = Dense(1, kernel_initializer = 'he_normal')(xz)\n",
    "        \n",
    "        s = Add()([sxz,sx,sz])\n",
    "\n",
    "        self.D = Model(inputs = [inp, inpl], outputs = s)\n",
    "\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "\n",
    "#         if self.G:\n",
    "#             return self.G\n",
    "\n",
    "        #Inputs\n",
    "        inp = Input(shape = [latent_size])\n",
    "\n",
    "        #Latent\n",
    "\n",
    "        #Actual Model\n",
    "        x = Dense(4*4*16*cha, kernel_initializer = 'he_normal')(inp)\n",
    "        x = Reshape([4, 4, 16*cha])(x)\n",
    "\n",
    "        x = g_block(x, 16 * cha)  #4\n",
    "        x = g_block(x, 16 * cha)  #8\n",
    "        x = g_block(x, 8 * cha)  #16\n",
    "        x = self_att_block_sn(x, 8 * cha)\n",
    "        x = g_block(x, 8 * cha)   #64\n",
    "        x = g_block(x, 4 * cha)   #128\n",
    "\n",
    "        x = Conv2D(filters = channels, kernel_size = 1, activation = 'tanh', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "        \n",
    "        return Model(inputs = inp, outputs = x)\n",
    "#         self.G = Model(inputs = inp, outputs = x)\n",
    "#         return self.G\n",
    "\n",
    "    def encoder(self):\n",
    "\n",
    "#         if self.E:\n",
    "#             return self.E\n",
    "\n",
    "        inp = Input(shape = [im_size, im_size, channels])\n",
    "\n",
    "        x = d_block(inp, 4 * cha)   #64\n",
    "        x = d_block(x, 8 * cha)   #32\n",
    "        x = self_att_block(x, 8 * cha)\n",
    "        x = d_block(x, 8 * cha)   #16\n",
    "        x = d_block(x, 16 * cha)  #8\n",
    "        x = d_block(x, 16 * cha)  #4\n",
    "#         x = d_block(x, 16 * cha, p = False)  #4\n",
    "\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "#         x = Dense(16 * cha, kernel_initializer = 'he_normal')(x)\n",
    "#         x = LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = Dense(latent_size, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(x)\n",
    "        return Model(inputs = inp, outputs = x)\n",
    "#         self.E = Model(inputs = inp, outputs = x)\n",
    "#         return self.E\n",
    "\n",
    "\n",
    "    def AdModel(self):\n",
    "\n",
    "        #D does not update\n",
    "        self.D.trainable = False\n",
    "        for layer in self.D.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        #G does update\n",
    "        self.G.trainable = True\n",
    "        for layer in self.G.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        #E does update\n",
    "        self.E.trainable = True\n",
    "        for layer in self.E.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # Fake Latent / Real Image\n",
    "        ri = Input(shape = [im_size, im_size, channels])\n",
    "\n",
    "        er = self.E(ri)\n",
    "        dr = self.D([ri, er])\n",
    "\n",
    "        # Real Latent / Fake Image\n",
    "        gi = Input(shape = [latent_size])\n",
    "\n",
    "        gf = self.G(gi)\n",
    "        df = self.D([gf, gi])\n",
    "\n",
    "        self.AM = Model(inputs = [ri, gi], outputs = [dr, df])\n",
    "        self.E_tr = Model(inputs = ri, outputs = dr)\n",
    "        self.G_tr = Model(inputs = gi, outputs = df)\n",
    "#         self.E_tr = multi_gpu_model(Model(inputs = ri, outputs = dr),2)\n",
    "#         self.G_tr = multi_gpu_model(Model(inputs = gi, outputs = df),2)\n",
    "\n",
    "\n",
    "        self.E_tr.compile(optimizer = Adam(self.LR*1.5, beta_1 = 0, beta_2 = 0.999,epsilon=adam_eps), loss = w_loss)\n",
    "        self.G_tr.compile(optimizer = Adam(self.LR*1.5, beta_1 = 0, beta_2 = 0.999,epsilon=adam_eps), loss = w_loss)\n",
    "#         self.AM.compile(optimizer = Adam(self.LR, beta_1 = 0, beta_2 = 0.999,epsilon=adam_eps), loss = [w_loss, w_loss])\n",
    "\n",
    "        return self.E_tr, self.G_tr\n",
    "\n",
    "    def DisModel(self):\n",
    "\n",
    "        #D does update\n",
    "        self.D.trainable = True\n",
    "        for layer in self.D.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        #G does not update\n",
    "        self.G.trainable = False\n",
    "        for layer in self.G.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        #E does update\n",
    "        self.E.trainable = False\n",
    "        for layer in self.E.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Fake Latent / Real Image\n",
    "        ri = Input(shape = [im_size, im_size, channels])\n",
    "\n",
    "        er = self.E(ri)\n",
    "        dr = self.D([ri, er])\n",
    "\n",
    "        # Real Latent / Fake Image\n",
    "        gi = Input(shape = [latent_size])\n",
    "\n",
    "        gf = self.G(gi)\n",
    "        df = self.D([gf, gi])\n",
    "\n",
    "        self.DM = Model(inputs = [ri, gi], outputs = [dr, df])\n",
    "#         self.DM = multi_gpu_model(Model(inputs = [ri, gi], outputs = [dr, df]),2)\n",
    "        self.DM.compile(optimizer = Adam(self.LR*1, beta_1 = 0, beta_2 = 0.999,epsilon=adam_eps),\n",
    "                        loss=[hinge_d, hinge_d])\n",
    "        return self.DM\n",
    "    \n",
    "    def EMA(self):\n",
    "\n",
    "        start = time.clock()\n",
    "\n",
    "        for i in range(len(self.G.layers)):\n",
    "            up_weight = self.G.layers[i].get_weights()\n",
    "            old_weight = self.GE.layers[i].get_weights()\n",
    "            new_weight = []\n",
    "            for j in range(len(up_weight)):\n",
    "                new_weight.append(old_weight[j] * self.beta + (1-self.beta) * up_weight[j])\n",
    "            self.GE.layers[i].set_weights(new_weight)\n",
    "\n",
    "        for i in range(len(self.E.layers)):\n",
    "            up_weight = self.E.layers[i].get_weights()\n",
    "            old_weight = self.EE.layers[i].get_weights()\n",
    "            new_weight = []\n",
    "            for j in range(len(up_weight)):\n",
    "                new_weight.append(old_weight[j] * self.beta + (1-self.beta) * up_weight[j])\n",
    "            self.EE.layers[i].set_weights(new_weight)\n",
    "\n",
    "#         print(\"Moved Average. \" + str(time.clock() - start) + \"s\")\n",
    "\n",
    "    def MAinit(self):\n",
    "        self.EE.set_weights(self.E.get_weights())\n",
    "        self.GE.set_weights(self.G.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Generate Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = ''\n",
    "df_have_fc_tr = pd.read_csv(path_1+'df_have_fc_tr.csv',index_col='index')\n",
    "df_have_fc_val = pd.read_csv(path_1+'df_have_fc_val.csv',index_col='index')\n",
    "fix_image = pd.read_csv(path_1+'fix_image.csv')\n",
    "dir_f = ''\n",
    "fix_image.tr = dir_f+fix_image.tr\n",
    "fix_image.val = dir_f+fix_image.val\n",
    "df_have_fc_tr.index = dir_f+df_have_fc_tr.index\n",
    "df_have_fc_val.index = dir_f+df_have_fc_val.index\n",
    "\n",
    "def ImageProcessing(path,resize=resize,cut_size=im_size,random_cut=True):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    # 把大小調到 400 * 400\n",
    "    img = cv2.resize(img, (resize, resize), interpolation=cv2.INTER_CUBIC)\n",
    "    (h, w) = img.shape[:2]\n",
    "    # 把尺裁掉\n",
    "    h = int(h*(9/10))\n",
    "    img = img[:h]\n",
    "    # 隨機水平翻轉\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    coin = np.random.randint(2)\n",
    "    if coin == 0:\n",
    "        img = cv2.flip(img, 1)\n",
    "    # 隨機裁減圖片\n",
    "    if random_cut==True:\n",
    "        h_s = np.random.randint(h-cut_size)\n",
    "        w_s = np.random.randint(w-cut_size)\n",
    "        img = img[h_s:h_s+cut_size,w_s:w_s+cut_size]\n",
    "    else:\n",
    "        ch,cw = img.shape[0]//2, img.shape[1]//2\n",
    "        img = img[ch-cut_size//2:ch+cut_size//2,cw-cut_size//2:cw+cut_size//2]\n",
    "    img = img.astype('float')\n",
    "    img-=127.5; img/=127.5\n",
    "\n",
    "    return img\n",
    "\n",
    "def gen_data(index_list):\n",
    "    res = []\n",
    "    for i in index_list:\n",
    "        res.append(ImageProcessing(i))\n",
    "    return np.r_[res]\n",
    "def gen_data_fix(index_list):\n",
    "    res = []\n",
    "    for i in index_list:\n",
    "        res.append(ImageProcessing_fix(i))\n",
    "    return np.r_[res]    \n",
    "def balance_data_gen(bs = BATCH_SIZE//15):\n",
    "    img_name = []\n",
    "    col = df_have_fc_tr.Layer_1.unique()\n",
    "    for i in col:\n",
    "        col_idx = df_have_fc_tr[df_have_fc_tr.Layer_1==i].index\n",
    "        img_name.append(np.random.choice(col_idx,size=bs,replace=True))\n",
    "    img_name = np.concatenate(img_name)\n",
    "    x_img = gen_data(img_name)\n",
    "#         print(img_name)\n",
    "    return x_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Build and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from keras.datasets import cifar10\n",
    "class BiGAN(object):\n",
    "\n",
    "    def __init__(self, steps = 1, lr = 0.0001, decay = 0.00001, silent = True):\n",
    "\n",
    "        self.GAN = GAN(steps = steps, lr = lr, decay = decay)\n",
    "        self.DisModel = self.GAN.DisModel()\n",
    "#         self.AdModel \n",
    "        self.E_tr, self.G_tr = self.GAN.AdModel()\n",
    "\n",
    "        \n",
    "        self.latent_dim = latent_size\n",
    "        self.batch_size=BATCH_SIZE\n",
    "        self.silent = silent\n",
    "\n",
    "        #Train Generator to be in the middle, not all the way at real. Apparently works better??\n",
    "        self.ones = np.ones((BATCH_SIZE, 1), dtype=np.float32)\n",
    "        self.zeros = np.zeros((BATCH_SIZE, 1), dtype=np.float32)\n",
    "        self.nones = -self.ones\n",
    "\n",
    "    \n",
    "    def train_fabric(self, epochs_last_time, epochs,dir1='sngan_plot_qq_1/',train_process_name='train_process'):\n",
    "        train_process = pd.DataFrame()\n",
    "#         train_process = pd.read_csv('sn-sa-bigan-new-128-sfat32-no-res/train_process2.csv')\n",
    "        if not os.path.isdir(dir1):\n",
    "            os.mkdir(dir1)\n",
    "\n",
    "            \n",
    "        fixed_real_img_idx_tr = fix_image.tr\n",
    "        fixed_real_img_idx_val = fix_image.val\n",
    "        fixed_real_img_tr = gen_data(fixed_real_img_idx_tr)\n",
    "        fixed_real_img_val = gen_data_fix(fixed_real_img_idx_val)                \n",
    "\n",
    "        \n",
    "\n",
    "        fixed_noise = np.random.normal(size=(40,self.latent_dim))\n",
    "        dummy_y = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        for epoch in tqdm_notebook(range(epochs_last_time,epochs)):\n",
    "        \n",
    "            # 真實圖片跟雜訊\n",
    "            \n",
    "            imgs = balance_data_gen()\n",
    "#             imgs = X_train[np.random.choice(tr_idx,size=self.batch_size,replace=False)]\n",
    "            noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "            \n",
    "        \n",
    "            train_data = [imgs,noise]\n",
    "            d_loss = self.DisModel.train_on_batch(train_data, [self.ones, self.nones])\n",
    "\n",
    "            ge_loss = self.E_tr.train_on_batch(imgs, self.ones)\n",
    "            gg_loss = self.G_tr.train_on_batch(noise, self.nones)\n",
    "#             g_loss = self.AdModel.train_on_batch(train_data, [self.ones, self.nones])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "            embed = self.GAN.E.predict(imgs)\n",
    "            mean_ = embed.mean()\n",
    "            std_ = embed.std()\n",
    "            std_2 = embed.std(axis=0).mean()\n",
    "            \n",
    "#             print('------epoch',epoch,'------')\n",
    "#             print('|d_loss: ',d_loss,'\\n|g_loss_noise: ',[ge_loss+gg_loss,ge_loss,gg_loss],'mean/std',[mean_,std_])\n",
    "#             print(embed.std(axis=0).shape)\n",
    "            \n",
    "            train_process = self.save_train_process(train_process,epoch,d_loss, [ge_loss+gg_loss,ge_loss,gg_loss],\n",
    "                                                    [mean_,std_,std_2])            \n",
    "            \n",
    "            path = dir1+train_process_name+'.csv'\n",
    "            train_process.to_csv(path)\n",
    "            \n",
    "            if epoch%10==0:\n",
    "                \n",
    "                self.GAN.EMA()\n",
    "\n",
    "            if epoch == 20000:\n",
    "                self.GAN.MAinit()\n",
    "            \n",
    "            if epoch%500==0:\n",
    "#                 recon_tr = self.generator.predict(self.encoder.predict(fixed_real_img_tr)[0])\n",
    "#                 recon_val = self.generator.predict(self.encoder.predict(fixed_real_img_val)[0])\n",
    "                recon_tr = self.GAN.G.predict(self.GAN.E.predict(fixed_real_img_tr))\n",
    "                recon_val = self.GAN.G.predict(self.GAN.E.predict(fixed_real_img_val))\n",
    "                fake_fix = self.GAN.G.predict(fixed_noise)\n",
    "                plt.figure(figsize=(60,40))        \n",
    "                for i in range(1,31):\n",
    "                    if i<=15:\n",
    "                        plt.subplot(10,15,i)\n",
    "                        plt.imshow((np.squeeze(fixed_real_img_tr[i-1])+1)/2)\n",
    "                        plt.subplot(10,15,i+15)\n",
    "                        plt.imshow((np.squeeze(recon_tr[i-1])+1)/2)\n",
    "                    else:\n",
    "                        plt.subplot(10,15,i+15)\n",
    "                        plt.imshow((np.squeeze(fixed_real_img_tr[i-1])+1)/2)\n",
    "                        plt.subplot(10,15,i+30)\n",
    "                        plt.imshow((np.squeeze(recon_tr[i-1])+1)/2)\n",
    "\n",
    "\n",
    "                for i in range(61,91):\n",
    "                    if i <=75:\n",
    "                        plt.subplot(10,15,i)\n",
    "                        plt.imshow((np.squeeze(fixed_real_img_val[i-61])+1)/2)\n",
    "                        plt.subplot(10,15,i+15)\n",
    "                        plt.imshow((np.squeeze(recon_val[i-61])+1)/2)\n",
    "                    else:\n",
    "                        plt.subplot(10,15,i+15)\n",
    "                        plt.imshow((np.squeeze(fixed_real_img_val[i-61])+1)/2)\n",
    "                        plt.subplot(10,15,i+30)\n",
    "                        plt.imshow((np.squeeze(recon_val[i-61])+1)/2)\n",
    "                    \n",
    "                for i in range(121,151):\n",
    "                    plt.subplot(10,15,i)\n",
    "                    plt.imshow((np.squeeze(fake_fix[i-121])+1)/2)\n",
    "                file = dir1+'20822iter_'+str(epoch)+'.jpg'\n",
    "                plt.savefig(file)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "                plt.figure(figsize=(10,5))\n",
    "                plt.hist(embed.ravel(),bins=100)\n",
    "                file = dir1+'20822iter_dist_'+str(epoch)+'.jpg'\n",
    "                plt.savefig(file)\n",
    "                plt.show()\n",
    "                plt.close()         \n",
    "                \n",
    "                self.GAN.E.save_weights(dir1+str(epoch)+'epoch_encoder.h5')\n",
    "                self.GAN.G.save_weights(dir1+str(epoch)+'epoch_generator.h5')\n",
    "                self.GAN.D.save_weights(dir1+str(epoch)+'epoch_critic_o.h5')\n",
    "                train_process = train_process[['d_loss', 'g_loss', 'd_real_score', 'd_fake_sore',\n",
    "                       'g_real_score', 'g_fake_score', 'mean', 'std','std_ef']]\n",
    "                col = train_process.columns                \n",
    "                display(train_process.iloc[::-1][:5])\n",
    "                plt.figure(figsize=(20,8))\n",
    "                for i in col:\n",
    "                    plt.plot(train_process[i])\n",
    "                plt.legend(col)\n",
    "                plt.show()\n",
    "                plt.close() \n",
    "                \n",
    "                col = ['d_loss', 'g_loss']             \n",
    "                display(train_process.iloc[::-1][:5])\n",
    "                plt.figure(figsize=(20,8))\n",
    "                for i in col:\n",
    "                    plt.plot(train_process[i])\n",
    "                plt.legend(col)\n",
    "                plt.show()\n",
    "                plt.close() \n",
    "                \n",
    "    def save_train_process(self,train_process,epoch,d_loss, g_loss,sta):\n",
    "        train_process.loc[epoch,'d_loss']=d_loss[0]\n",
    "        train_process.loc[epoch,'d_real_score']=1-d_loss[1]\n",
    "        train_process.loc[epoch,'d_fake_sore']=d_loss[2]-1\n",
    "#         train_process.loc[epoch,'d_wgp_f1_loss']=d_loss[3]\n",
    "#         train_process.loc[epoch,'d_wgp_loss']=d_loss[4]\n",
    "        \n",
    "        train_process.loc[epoch,'g_loss']=g_loss[0]\n",
    "        train_process.loc[epoch,'g_real_score']=g_loss[1]\n",
    "        train_process.loc[epoch,'g_fake_score']=-g_loss[2]\n",
    "        train_process.loc[epoch,'mean']=sta[0]\n",
    "        train_process.loc[epoch,'std']=sta[1]\n",
    "        train_process.loc[epoch,'std_ef']=sta[2]\n",
    "        \n",
    "        return train_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "sngan = BiGAN(lr=0.0001)\n",
    "# sngan.critic_o.summary()\n",
    "dir_name='sn-sa-bigan-new-128-sfat32-use-res-3-conv-bn-act-gpuno2'+'/'\n",
    "sngan.train_fabric(epochs_last_time=0,epochs=1000000,dir1=dir_name,train_process_name='train_process2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine import *\n",
    "from keras.legacy import interfaces\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.utils.generic_utils import func_dump\n",
    "from keras.utils.generic_utils import func_load\n",
    "from keras.utils.generic_utils import deserialize_keras_object\n",
    "from keras.utils.generic_utils import has_arg\n",
    "from keras.utils import conv_utils\n",
    "from keras.legacy import interfaces\n",
    "from keras.layers import Dense, Conv1D, Conv2D, Conv3D, Conv2DTranspose, Embedding\n",
    "import tensorflow as tf\n",
    "\n",
    "class DenseSN(Dense):\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                                 initializer=initializers.RandomNormal(0, 1),\n",
    "                                 name='sn',\n",
    "                                 trainable=False)\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                 W_bar = K.reshape(W_bar, W_shape)  \n",
    "        output = K.dot(inputs, W_bar)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output \n",
    "        \n",
    "class _ConvSN(Layer):\n",
    "\n",
    "    def __init__(self, rank,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=1,\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=1,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 spectral_normalization=True,\n",
    "                 **kwargs):\n",
    "        super(_ConvSN, self).__init__(**kwargs)\n",
    "        self.rank = rank\n",
    "        self.filters = filters\n",
    "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
    "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
    "        self.padding = conv_utils.normalize_padding(padding)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2)\n",
    "        self.spectral_normalization = spectral_normalization\n",
    "        self.u = None\n",
    "        \n",
    "    def _l2normalize(self, v, eps=1e-12):\n",
    "        return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "    \n",
    "    def power_iteration(self, u, W):\n",
    "        '''\n",
    "        Accroding the paper, we only need to do power iteration one time.\n",
    "        '''\n",
    "        v = self._l2normalize(K.dot(u, K.transpose(W)))\n",
    "        u = self._l2normalize(K.dot(v, W))\n",
    "        return u, v\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        #Spectral Normalization\n",
    "        if self.spectral_normalization:\n",
    "            self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                                     initializer=initializers.RandomNormal(0, 1),\n",
    "                                     name='sn',\n",
    "                                     trainable=False)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        \n",
    "        if self.spectral_normalization:\n",
    "            W_shape = self.kernel.shape.as_list()\n",
    "            #Flatten the Tensor\n",
    "            W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "            _u, _v = power_iteration(W_reshaped, self.u)\n",
    "            #Calculate Sigma\n",
    "            sigma=K.dot(_v, W_reshaped)\n",
    "            sigma=K.dot(sigma, K.transpose(_u))\n",
    "            #normalize it\n",
    "            W_bar = W_reshaped / sigma\n",
    "            #reshape weight tensor\n",
    "            if training in {0, False}:\n",
    "                W_bar = K.reshape(W_bar, W_shape)\n",
    "            else:\n",
    "                with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                    W_bar = K.reshape(W_bar, W_shape)\n",
    "\n",
    "            #update weitht\n",
    "            self.kernel = W_bar\n",
    "        \n",
    "        if self.rank == 1:\n",
    "            outputs = K.conv1d(\n",
    "                inputs,\n",
    "                self.kernel,\n",
    "                strides=self.strides[0],\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate[0])\n",
    "        if self.rank == 2:\n",
    "            outputs = K.conv2d(\n",
    "                inputs,\n",
    "                self.kernel,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "        if self.rank == 3:\n",
    "            outputs = K.conv3d(\n",
    "                inputs,\n",
    "                self.kernel,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_last':\n",
    "            space = input_shape[1:-1]\n",
    "            new_space = []\n",
    "            for i in range(len(space)):\n",
    "                new_dim = conv_utils.conv_output_length(\n",
    "                    space[i],\n",
    "                    self.kernel_size[i],\n",
    "                    padding=self.padding,\n",
    "                    stride=self.strides[i],\n",
    "                    dilation=self.dilation_rate[i])\n",
    "                new_space.append(new_dim)\n",
    "            return (input_shape[0],) + tuple(new_space) + (self.filters,)\n",
    "        if self.data_format == 'channels_first':\n",
    "            space = input_shape[2:]\n",
    "            new_space = []\n",
    "            for i in range(len(space)):\n",
    "                new_dim = conv_utils.conv_output_length(\n",
    "                    space[i],\n",
    "                    self.kernel_size[i],\n",
    "                    padding=self.padding,\n",
    "                    stride=self.strides[i],\n",
    "                    dilation=self.dilation_rate[i])\n",
    "                new_space.append(new_dim)\n",
    "            return (input_shape[0], self.filters) + tuple(new_space)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'rank': self.rank,\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'strides': self.strides,\n",
    "            'padding': self.padding,\n",
    "            'data_format': self.data_format,\n",
    "            'dilation_rate': self.dilation_rate,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(_Conv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "class ConvSN2D(Conv2D):\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                         initializer=initializers.RandomNormal(0, 1),\n",
    "                         name='sn',\n",
    "                         trainable=False)\n",
    "        \n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "    def call(self, inputs, training=None):\n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            #Accroding the paper, we only need to do power iteration one time.\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        #Spectral Normalization\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                W_bar = K.reshape(W_bar, W_shape)\n",
    "                \n",
    "        outputs = K.conv2d(\n",
    "                inputs,\n",
    "                W_bar,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class ConvSN1D(Conv1D):\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                 initializer=initializers.RandomNormal(0, 1),\n",
    "                 name='sn',\n",
    "                 trainable=False)\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            #Accroding the paper, we only need to do power iteration one time.\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        #Spectral Normalization\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                W_bar = K.reshape(W_bar, W_shape)\n",
    "                \n",
    "        outputs = K.conv1d(\n",
    "                inputs,\n",
    "                W_bar,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "class ConvSN3D(Conv3D):    \n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        \n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                         initializer=initializers.RandomNormal(0, 1),\n",
    "                         name='sn',\n",
    "                         trainable=False)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            #Accroding the paper, we only need to do power iteration one time.\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        #Spectral Normalization\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                W_bar = K.reshape(W_bar, W_shape)\n",
    "                \n",
    "        outputs = K.conv3d(\n",
    "                inputs,\n",
    "                W_bar,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "        \n",
    "class EmbeddingSN(Embedding):\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self.input_dim, self.output_dim),\n",
    "            initializer=self.embeddings_initializer,\n",
    "            name='embeddings',\n",
    "            regularizer=self.embeddings_regularizer,\n",
    "            constraint=self.embeddings_constraint,\n",
    "            dtype=self.dtype)\n",
    "        \n",
    "        self.u = self.add_weight(shape=tuple([1, self.embeddings.shape.as_list()[-1]]),\n",
    "                         initializer=initializers.RandomNormal(0, 1),\n",
    "                         name='sn',\n",
    "                         trainable=False)\n",
    "        \n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if K.dtype(inputs) != 'int32':\n",
    "            inputs = K.cast(inputs, 'int32')\n",
    "            \n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            #Accroding the paper, we only need to do power iteration one time.\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        W_shape = self.embeddings.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.embeddings, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                W_bar = K.reshape(W_bar, W_shape)\n",
    "        self.embeddings = W_bar\n",
    "            \n",
    "        out = K.gather(self.embeddings, inputs)\n",
    "        return out \n",
    "\n",
    "class ConvSN2DTranspose(Conv2DTranspose):\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) != 4:\n",
    "            raise ValueError('Inputs should have rank ' +\n",
    "                             str(4) +\n",
    "                             '; Received input shape:', str(input_shape))\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (self.filters, input_dim)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                         initializer=initializers.RandomNormal(0, 1),\n",
    "                         name='sn',\n",
    "                         trainable=False)\n",
    "        \n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n",
    "        self.built = True  \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_shape = K.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        if self.data_format == 'channels_first':\n",
    "            h_axis, w_axis = 2, 3\n",
    "        else:\n",
    "            h_axis, w_axis = 1, 2\n",
    "\n",
    "        height, width = input_shape[h_axis], input_shape[w_axis]\n",
    "        kernel_h, kernel_w = self.kernel_size\n",
    "        stride_h, stride_w = self.strides\n",
    "        if self.output_padding is None:\n",
    "            out_pad_h = out_pad_w = None\n",
    "        else:\n",
    "            out_pad_h, out_pad_w = self.output_padding\n",
    "\n",
    "        # Infer the dynamic output shape:\n",
    "        out_height = conv_utils.deconv_length(height,\n",
    "                                              stride_h, kernel_h,\n",
    "                                              self.padding,\n",
    "                                              out_pad_h)\n",
    "        out_width = conv_utils.deconv_length(width,\n",
    "                                             stride_w, kernel_w,\n",
    "                                             self.padding,\n",
    "                                             out_pad_w)\n",
    "        if self.data_format == 'channels_first':\n",
    "            output_shape = (batch_size, self.filters, out_height, out_width)\n",
    "        else:\n",
    "            output_shape = (batch_size, out_height, out_width, self.filters)\n",
    "            \n",
    "        #Spectral Normalization    \n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            #Accroding the paper, we only need to do power iteration one time.\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                W_bar = K.reshape(W_bar, W_shape)\n",
    "        self.kernel = W_bar\n",
    "        \n",
    "        outputs = K.conv2d_transpose(\n",
    "            inputs,\n",
    "            self.kernel,\n",
    "            output_shape,\n",
    "            self.strides,\n",
    "            padding=self.padding,\n",
    "            data_format=self.data_format)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "目錄",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "278px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
